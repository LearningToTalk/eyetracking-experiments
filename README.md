# L2T Eyetracking Experiments

This repository holds the eyetracking experiments we used in the 
Learning To Talk lab. We used Eprime with a Tobii eyetracker, so 
the actual experiments require proprietary software to open.

TJ Mahr created this repository well after all the data had been 
collected so that he could keep straight which recordings were 
used with which experiments. We had never used a proper version 
control system on our experiments.

For version control efficiency, movie files and the sound effects 
for the puzzle-game have not been added to the 
repository even though they are a minor part of the experiments.

## Abbreviations

Experiment type: 

- MP: Our two-image mispronunciation (MP) experiment.
- RWL: Our four-image "visual world" experiment. (RWL was short for Real 
  Word Listening, because they did not hear any nonwords or 
  mispronunciations).

Dialects:

- AAE: Experiment used a speaker of African American English.
- SAE: Experiment used a speaker of Mainstream American English. 

The S was for Standard but we've moved away from that terminology, but 
our filenames have that vestigial abbreviation.

## Cross-sectional 1 (circa 2012) [`cs1`]

These are the first eyetracking experiments we did. They do not feature 
any gaze-contingent stimulus presentation. 

The MAE Mispronunciation experiment here is the basis for Law and Edwards (2015).

## Cross-sectional 2 (circa 2012) [`cs2`]

This next batch of experiments incorporated an orienting stimulus (a 
small animated image at the center of the screen) to prevent the 
listener from already being fixated a named image when the word is 
produced. By controlling gazes in this way, we figured we could get more 
reaction time measurements. 

## TimePoint 1 (circa (2013)

These are the experiments from the first year of our longitudinal study. 
There was a timing glitch with the experiments for first few participants. 
This experiment used a gaze-contingent stimulus presentation, but it only 
required the child to look the screen instead of a specific region of the 
screen. We tried different tweaks to the experiment, including a version 
with a gray background to see if that improved eyetracking. We also added 
an option to use a different calibration procedure (with a "ducky" movie).

## TimePoint 2 (circa (2014)

We used newly recording stimuli so that two dialect versions of the experiment 
had similar durations.

## TimePoint 3 (circa 2015) 

We got rid of the final attention getter phrase (e.g., "check it out!") from 
the end of each trial to speed up the experiment. We also replace the 
movies that would play every 6 or 7 trials with a gaze contingency game in 
a child's gaze would remove puzzle pieces to reveal an image.